{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vivekdesai25/Evoastra_Internship/blob/main/Deep_Learning_(vivek)_Major_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ojaswi's part"
      ],
      "metadata": {
        "id": "F5kajKQB9eX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Download and Set Up the MS COCO Dataset"
      ],
      "metadata": {
        "id": "fVGgSoBt9qR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download annotations zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "\n",
        "# Unzip it\n",
        "!unzip annotations_trainval2017.zip -d coco_annotations\n",
        "\n",
        "# Confirm the file is there\n",
        "import os\n",
        "print(\"Files in annotations folder:\", os.listdir(\"coco_annotations/annotations\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn98N9G991WE",
        "outputId": "6a237e2f-a9f9-4a32-ab1d-ab14800776ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-02 12:13:12--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.116.65, 16.15.193.148, 3.5.12.238, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.116.65|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  96.2MB/s    in 2.5s    \n",
            "\n",
            "2025-08-02 12:13:15 (96.2 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n",
            "Archive:  annotations_trainval2017.zip\n",
            "  inflating: coco_annotations/annotations/instances_train2017.json  \n",
            "  inflating: coco_annotations/annotations/instances_val2017.json  \n",
            "  inflating: coco_annotations/annotations/captions_train2017.json  \n",
            "  inflating: coco_annotations/annotations/captions_val2017.json  \n",
            "  inflating: coco_annotations/annotations/person_keypoints_train2017.json  \n",
            "  inflating: coco_annotations/annotations/person_keypoints_val2017.json  \n",
            "Files in annotations folder: ['person_keypoints_val2017.json', 'instances_val2017.json', 'person_keypoints_train2017.json', 'instances_train2017.json', 'captions_val2017.json', 'captions_train2017.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load & Parse the Caption File"
      ],
      "metadata": {
        "id": "xtXm4GhS999n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Path to JSON caption file\n",
        "caption_file_path = \"coco_annotations/annotations/captions_train2017.json\"\n",
        "\n",
        "# Load JSON data\n",
        "with open(caption_file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Create dictionary to hold image_id to list of captions\n",
        "captions_dict = defaultdict(list)\n",
        "\n",
        "# Loop through all caption annotations\n",
        "for annot in data['annotations']:\n",
        "    image_id = annot['image_id']\n",
        "    caption = annot['caption']\n",
        "\n",
        "    # Convert image_id (int) to filename format (str)\n",
        "    image_filename = f\"{image_id:012d}.jpg\"\n",
        "\n",
        "    # Append the caption to corresponding image key\n",
        "    captions_dict[image_filename].append(caption)\n",
        "\n",
        "# check number of images and a sample\n",
        "print(\"Total unique images with captions:\", len(captions_dict))\n",
        "sample_key = list(captions_dict.keys())[0]\n",
        "print(f\"\\nSample image filename: {sample_key}\")\n",
        "print(f\"Captions for this image:\\n{captions_dict[sample_key]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq-RWStG-GRS",
        "outputId": "a89c10d9-c55b-4f32-a7e8-728b93bb6e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique images with captions: 118287\n",
            "\n",
            "Sample image filename: 000000203564.jpg\n",
            "Captions for this image:\n",
            "['A bicycle replica with a clock as the front wheel.', 'The bike has a clock as a tire.', 'A black metal bicycle with a clock inside the front wheel.', 'A bicycle figurine in which the front wheel is replaced with a clock\\n', 'A clock with the appearance of the wheel of a bicycle ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Clean and Preprocess Captions"
      ],
      "metadata": {
        "id": "gtR0Dixv-pBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "\n",
        "# Load caption file again\n",
        "with open(\"coco_annotations/annotations/captions_train2017.json\", \"r\") as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# Recreate image_id to captions mapping\n",
        "image_captions = {}\n",
        "for item in captions_data[\"annotations\"]:\n",
        "    img_id = f\"{item['image_id']:012d}.jpg\"\n",
        "    caption = item[\"caption\"]\n",
        "    if img_id not in image_captions:\n",
        "        image_captions[img_id] = []\n",
        "    image_captions[img_id].append(caption)\n",
        "\n",
        "# Clean each caption\n",
        "def clean_caption(caption):\n",
        "    caption = caption.lower()  # lowercase\n",
        "    caption = caption.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
        "    caption = ' '.join([word for word in caption.split() if word.isalpha()])  # remove non-alpha\n",
        "    caption = '<start> ' + caption + ' <end>'  # add start/end\n",
        "    return caption\n",
        "\n",
        "# Clean all captions\n",
        "cleaned_captions = {}\n",
        "for img_id, caption_list in image_captions.items():\n",
        "    cleaned_captions[img_id] = [clean_caption(c) for c in caption_list]\n",
        "\n",
        "# Print\n",
        "sample_id = list(cleaned_captions.keys())[0]\n",
        "print(\"Image:\", sample_id)\n",
        "print(\"Cleaned Captions:\")\n",
        "for c in cleaned_captions[sample_id]:\n",
        "    print(c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03koxmv5-v6Y",
        "outputId": "63219b38-2743-4dce-9d4b-69fb3f215237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 000000203564.jpg\n",
            "Cleaned Captions:\n",
            "<start> a bicycle replica with a clock as the front wheel <end>\n",
            "<start> the bike has a clock as a tire <end>\n",
            "<start> a black metal bicycle with a clock inside the front wheel <end>\n",
            "<start> a bicycle figurine in which the front wheel is replaced with a clock <end>\n",
            "<start> a clock with the appearance of the wheel of a bicycle <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Tokenize and Convert Captions to Sequences"
      ],
      "metadata": {
        "id": "gKVn6U_ggcAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Flatten all captions into single list\n",
        "all_captions = []\n",
        "for cap_list in cleaned_captions.values():\n",
        "    all_captions.extend(cap_list)\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "\n",
        "# Vocabulary size (+1 for padding)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert captions to sequences\n",
        "caption_seqs = {}\n",
        "for img_id, cap_list in cleaned_captions.items():\n",
        "    caption_seqs[img_id] = tokenizer.texts_to_sequences(cap_list)\n",
        "\n",
        "# Check max length of any caption\n",
        "max_length = max(len(seq) for cap_list in caption_seqs.values() for seq in cap_list)\n",
        "\n",
        "# Show results\n",
        "print(f\"Total words in vocab: {vocab_size}\")\n",
        "print(f\"Max caption length: {max_length}\")\n",
        "\n",
        "# Sample\n",
        "sample_img = list(caption_seqs.keys())[0]\n",
        "print(f\"\\nImage: {sample_img}\")\n",
        "print(\"Tokenized captions:\")\n",
        "for seq in caption_seqs[sample_img]:\n",
        "    print(seq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "wYEGbqqK_FYK",
        "outputId": "8e397c29-bf30-47fd-b09a-1ff0366fd0f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cleaned_captions' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1351928275.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Flatten all captions into single list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mall_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcap_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned_captions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mall_captions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cleaned_captions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Pad the Sequences"
      ],
      "metadata": {
        "id": "1CghJp7n_T8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Store tokenized captions properly\n",
        "tokenized_captions = {}\n",
        "\n",
        "for img_id, caps in cleaned_captions.items():\n",
        "    tokenized = tokenizer.texts_to_sequences(caps)\n",
        "    tokenized_captions[img_id] = tokenized\n",
        "\n",
        "# Define max_len\n",
        "max_len = max(len(seq) for cap_list in tokenized_captions.values() for seq in cap_list)\n",
        "\n",
        "# Pad sequences\n",
        "padded_captions = {}\n",
        "\n",
        "for img_id, cap_seqs in tokenized_captions.items():\n",
        "    padded_captions[img_id] = pad_sequences(cap_seqs, maxlen=max_len, padding='post')\n"
      ],
      "metadata": {
        "id": "tatwRVRE_Vv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Print**"
      ],
      "metadata": {
        "id": "d489KdC4_cv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_img = next(iter(padded_captions))\n",
        "print(f\"Image: {sample_img}\")\n",
        "print(\"Padded Captions Shape:\", padded_captions[sample_img].shape)\n",
        "print(\"First Padded Caption:\", padded_captions[sample_img][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pmY-vHh_e_U",
        "outputId": "f38682ab-41a8-42a2-c072-5ce86b3ded61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 000000203564.jpg\n",
            "Padded Captions Shape: (5, 51)\n",
            "First Padded Caption: [   4    2  353 3787    9    2   83  122    7   40 1083    3    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Save Preprocessed Captions + Tokenizer"
      ],
      "metadata": {
        "id": "WtEAkzp5_pZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save tokenizer\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# Save padded captions\n",
        "with open('padded_captions.pkl', 'wb') as f:\n",
        "    pickle.dump(padded_captions, f)\n",
        "\n",
        "# Save cleaned captions\n",
        "with open('cleaned_captions.pkl', 'wb') as f:\n",
        "    pickle.dump(cleaned_captions, f)\n",
        "\n",
        "# Save max caption length\n",
        "with open('max_len.txt', 'w') as f:\n",
        "    f.write(str(max_len))\n"
      ],
      "metadata": {
        "id": "YQOkvMrm_tLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Check if it's saved**"
      ],
      "metadata": {
        "id": "9428jDqx_x_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Files saved in your working directory:\")\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4H45ThU_0EI",
        "outputId": "a0ce0ba0-f87f-4347-ff01-20c7b0866bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files saved in your working directory:\n",
            "['.config', 'tokenizer.pkl', 'coco_annotations', 'padded_captions.pkl', 'cleaned_captions.pkl', 'annotations_trainval2017.zip', 'max_len.txt', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7:Feature Extraction(Inception V3)"
      ],
      "metadata": {
        "id": "Kpnl_1VwpXV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Required Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 3: Load Pretrained InceptionV3 (CNN) Model\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
        "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
        "\n",
        "# Step 4: Preprocessing Function\n",
        "def load_preprocessed_img(img_path):\n",
        "    img = image.load_img(img_path, target_size=(299, 299))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "    return img_array\n",
        "\n",
        "# Step 5: Feature Extraction Function\n",
        "def extract_features_preprocessed(img_folder):\n",
        "    features = {}\n",
        "    # Get list of files in the folder\n",
        "    image_files = [f for f in os.listdir(img_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    for img_name in tqdm(image_files):\n",
        "        img_path = os.path.join(img_folder, img_name)\n",
        "        img_input = load_preprocessed_img(img_path)\n",
        "        feature = model.predict(img_input, verbose=0)\n",
        "        features[img_name] = feature.reshape(-1)\n",
        "    return features\n",
        "\n",
        "# Step 6: Set Folder Path on Google Drive (Update this path if needed)\n",
        "img_folder ='/content/drive/MyDrive/resized_images'\n",
        "\n",
        "# Step 7: Extract Features\n",
        "features = extract_features_preprocessed(img_folder)\n",
        "\n",
        "# Step 8: Save Extracted Features to Google Drive\n",
        "output_path = '/content/drive/MyDrive/image_features.pkl'\n",
        "with open(output_path, 'wb') as f:\n",
        "    pickle.dump(features, f)\n",
        "\n",
        "print(\"CNN Feature extraction completed and saved to Drive!\")"
      ],
      "metadata": {
        "id": "a8dD0klAGbo_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "9e6c3b21-bb5c-42c8-e67c-80a51c0a75a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/resized_images'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3961827120.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Step 7: Extract Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features_preprocessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Step 8: Save Extracted Features to Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3961827120.py\u001b[0m in \u001b[0;36mextract_features_preprocessed\u001b[0;34m(img_folder)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Get list of files in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/resized_images'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Go to Google Drive in Browser**\n",
        "\n",
        "- Open https://drive.google.com/drive/my-drive\n",
        "- Look for the folder `resized_images` in your **\"My Drive\"** section.\n",
        "\n",
        "#### If you don’t see it, then it means that you have not added it to your own Drive yet.\n",
        "-----------------------------------------------------------------------------\n",
        "### How to add it:\n",
        "\n",
        "To make Collab see the folder:\n",
        "\n",
        "1. **Open the Drive link** Aditya shared.\n",
        "2. Click **\"Add Shortcut to Drive\"** (or \"Add to Drive\").\n",
        "3. Choose: **My Drive** (not \"Shared with me\").\n",
        "3. Now it’ll appear in your \"My Drive\".\n",
        "4. Refresh your Google Drive tab to check it's really there.\n",
        "\n",
        "-----------------------------------------------------------------------------\n",
        "\n",
        "Run the below code and see in the output if you can see one of the file's name's is 'resized_images'\n",
        "\n",
        "If you can see, it means that the folder Aditya shared has been added into your google drive."
      ],
      "metadata": {
        "id": "_BGiSRqCQ9tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive\"))\n"
      ],
      "metadata": {
        "id": "4qLRdCAyQs0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP:8 LSTM CAPTIONING MODEL.**\n",
        "\n"
      ],
      "metadata": {
        "id": "MLab3695d-PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Parameters (adjust according to your tokenizer and dataset)\n",
        "vocab_size = 5000         # Replace with actual tokenizer.num_words or len(tokenizer.word_index) + 1\n",
        "max_length = 34           # Replace with actual max caption length\n",
        "embedding_dim = 256       # Size of word embeddings\n",
        "\n",
        "# Define the model\n",
        "def define_captioning_model(vocab_size, max_length):\n",
        "    # Feature extractor (image vector input)\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(embedding_dim, activation='relu')(fe1)\n",
        "\n",
        "    # Sequence processor (caption input)\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "\n",
        "    # Decoder (merge image + text)\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = define_captioning_model(vocab_size, max_length)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "r4-76MqfeIN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP:9 CAPTION GENERATION**"
      ],
      "metadata": {
        "id": "LACpkTGqizJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def generate_caption(model, tokenizer, photo, max_length):\n",
        "    in_text = '<start>'\n",
        "    for _ in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo, sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = tokenizer.index_word.get(yhat, None)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == '<end>':\n",
        "            break\n",
        "    final_caption = in_text.replace('<start>', '').replace('<end>', '').strip()\n",
        "    return final_caption"
      ],
      "metadata": {
        "id": "kMex7x5yjFPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP:10 USING WITH FEATURES.**"
      ],
      "metadata": {
        "id": "CVUqsJzDjW0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load image features\n",
        "with open('/content/drive/MyDrive/image_features.pkl', 'rb') as f:\n",
        "    features = pickle.load(f)\n",
        "\n",
        "# Choose one image\n",
        "img_name = list(features.keys())[0]\n",
        "photo_feature = features[img_name].reshape((1, 2048))  # Reshape for model input\n",
        "\n",
        "# Generate caption\n",
        "caption = generate_caption(model, tokenizer, photo_feature, max_length)\n",
        "print(f\"Caption for {img_name}: {caption}\")"
      ],
      "metadata": {
        "id": "moDy9fY1jhci",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "91f28566-02b3-49e2-ece4-a06cf0a38565"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/image_features.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-429220163.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load image features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/image_features.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/image_features.pkl'"
          ]
        }
      ]
    }
  ]
}